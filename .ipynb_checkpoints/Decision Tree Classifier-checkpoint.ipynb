{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "729db8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "import statistics\n",
    "from collections import deque\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "array = np.array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39c9eead",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTreeClassifier:\n",
    "    def __init__(self, max_depth = 5):\n",
    "        self.is_trained = False\n",
    "        self.tree = []\n",
    "        self.root = None\n",
    "        self.max_depth = max_depth\n",
    "    def fit(self, X, y):\n",
    "        self.root = Node(X,y)\n",
    "        to_be_split = deque()\n",
    "        to_be_split.append(self.root)\n",
    "        while len(to_be_split) > 0:\n",
    "            node = to_be_split.popleft()\n",
    "            self.tree.append(node)\n",
    "            if node.tree_level == self.max_depth:\n",
    "                node.is_leaf = True\n",
    "                continue\n",
    "            else:\n",
    "                node.best_split()\n",
    "                if node.has_split == True:\n",
    "                    to_be_split.append(node.left_branch)\n",
    "                    to_be_split.append(node.right_branch)\n",
    "        self.is_trained = True\n",
    "    def predict(self, X):\n",
    "        if len(X) == 1:\n",
    "            predictions = self.predict_one(X)\n",
    "        else:\n",
    "            predictions = [self.predict_one(x) for x in X]\n",
    "        return predictions\n",
    "    def predict_one(self, x):\n",
    "        if self.is_trained == False:\n",
    "            print('You need to train the model first!')\n",
    "        else:\n",
    "            currentNode = self.root\n",
    "            while currentNode.is_leaf == False:\n",
    "                split_idx, split_thresh = currentNode.split_on\n",
    "                if x[split_idx] < split_thresh:\n",
    "                    currentNode = currentNode.left_branch\n",
    "                else:\n",
    "                    currentNode = currentNode.right_branch\n",
    "            return statistics.mode(currentNode.y)\n",
    "        \n",
    "class Node:\n",
    "    def __init__(self, X, y, tree_level=0):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.num_features = self.X.shape[1]\n",
    "        self.num_samples_per_class = array([sum(self.y == C) for C in classes])\n",
    "        self.G = gini_impurity(self.num_samples_per_class)\n",
    "        self.has_split = False\n",
    "        self.split_on = None\n",
    "        self.left_branch = None\n",
    "        self.right_branch = None\n",
    "        self.child_G = 1\n",
    "        self.is_leaf = False\n",
    "        self.tree_level = tree_level\n",
    "\n",
    "    def best_split(self):\n",
    "        best_G = 1\n",
    "        split_idx = None\n",
    "        split_thresh = None\n",
    "        \n",
    "        for feat_idx in range(self.num_features):\n",
    "            thresh, G = best_split_for_feature(self, feat_idx)\n",
    " \n",
    "            if G < best_G:\n",
    "                best_G = G\n",
    "                split_idx = feat_idx\n",
    "                split_thresh = thresh\n",
    "\n",
    "        if best_G < self.G:\n",
    "            split(self, split_idx, split_thresh)\n",
    "            self.split_on = (split_idx, split_thresh)\n",
    "        else:\n",
    "            self.is_leaf = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ef3c278",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Some helper functions\n",
    "\n",
    "#Split parent_Node on split_idx against split_thresh\n",
    "#Create left and right branch and compute the resulting gini impurity \n",
    "def split(parent_Node, split_idx, split_thresh):\n",
    "    left_idx = parent_Node.X[:,split_idx] < split_thresh\n",
    "    right_idx = np.logical_not(left_idx)\n",
    "    left_X, left_y = parent_Node.X[left_idx], parent_Node.y[left_idx]\n",
    "    right_X, right_y = parent_Node.X[right_idx], parent_Node.y[right_idx]\n",
    "    parent_Node.left_branch = Node(left_X, left_y, parent_Node.tree_level+1)\n",
    "    parent_Node.right_branch = Node(right_X, right_y, parent_Node.tree_level+1)\n",
    "    parent_Node.has_split = True\n",
    "    gini_impurity_of_split(parent_Node)\n",
    "\n",
    "#Find the threshold value that leads to smallest gini impurity for feat_idx\n",
    "def best_split_for_feature(node, feat_idx):\n",
    "    node_copy = copy.deepcopy(node)\n",
    "    feature_array = node_copy.X[:,feat_idx]\n",
    "    thresh_vals = np.unique(feature_array)\n",
    "    best_thresh = None\n",
    "    best_G = 1\n",
    "    for thresh_val in thresh_vals:\n",
    "        split(node_copy, feat_idx, thresh_val)\n",
    "        if node_copy.child_G < best_G:\n",
    "            best_G = node_copy.child_G\n",
    "            best_thresh = thresh_val\n",
    "    return best_thresh, best_G\n",
    "\n",
    "#Gini impurity of a sample set is the probability of misclassiying a random sample\n",
    "#that is labeled according to the probability distribution of labels in the set.\n",
    "#Sum of (probability of class C)*(probability not of class C) across all Cs.\n",
    "#(1-pk)pk reduces to 1-pk^2 as the sum of pk across all C is 1.\n",
    "\n",
    "def gini_impurity(num_samples_per_class):\n",
    "    num_samples = sum(num_samples_per_class)\n",
    "    if num_samples == 0:\n",
    "        G = 1\n",
    "    else:\n",
    "        G = 1 - sum([(n_k/num_samples)**2 for n_k in num_samples_per_class])\n",
    "    return G\n",
    "\n",
    "#The gini impurity of a split is the weighted average of the gini impurities\n",
    "\n",
    "def gini_impurity_of_split(node):\n",
    "    if node.has_split == False:\n",
    "        pass\n",
    "    else:\n",
    "        left_y = node.left_branch.y\n",
    "        right_y = node.right_branch.y\n",
    "        num_samples = len(left_y) + len(right_y)\n",
    "        L_num_samples_per_class = array([sum(left_y == C) for C in classes])\n",
    "        R_num_samples_per_class = array([sum(right_y == C) for C in classes])\n",
    "        num_L_samples = sum(L_num_samples_per_class)\n",
    "        num_R_samples = sum(R_num_samples_per_class)\n",
    "        gini_L = gini_impurity(L_num_samples_per_class)\n",
    "        gini_R = gini_impurity(R_num_samples_per_class)\n",
    "        G = (gini_L*num_L_samples+gini_R*num_R_samples)/num_samples\n",
    "        node.child_G = G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0299c22a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[14,  0,  0],\n",
       "       [ 0, 10,  1],\n",
       "       [ 0,  0, 13]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = datasets.load_iris()\n",
    "X,y = data.data, data.target\n",
    "classes = set(y)\n",
    "\n",
    "trainX, valX, trainy, valy = train_test_split(X,y)\n",
    "myTreeClassifier = DecisionTreeClassifier()\n",
    "myTreeClassifier.fit(trainX,trainy)\n",
    "\n",
    "predictions = myTreeClassifier.predict(valX)\n",
    "\n",
    "confusion_matrix(valy, predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
